{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkWt8C7XOz2_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import math\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data():\n",
        "  pass\n",
        "\n",
        "def train_test_split():\n",
        "  pass\n",
        "\n",
        "X, y = load_data()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
        "#number of instances\n",
        "m = X_train.shape[0]\n",
        "#number of features\n",
        "n = X_train.shape[1]\n",
        "\n",
        "#Visualize data ...\n",
        "\n",
        "#initialize coefficient\n",
        "w = np.ones(n)"
      ],
      "metadata": {
        "id": "wZqW-m9uPDGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For loop\n",
        "def forward_loop(X,w):\n",
        "  m, n = X.shape\n",
        "  fx = []\n",
        "  for i in range(m):\n",
        "    fi = 0\n",
        "    for j in range(n):\n",
        "      fi += w[j] * X[i,j]\n",
        "    fx.append(fi)\n",
        "  return fx\n",
        "\n",
        "#Vectorization\n",
        "def forward_vector(X,w):\n",
        "  return X@w"
      ],
      "metadata": {
        "id": "LDNPCJnYU8WN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For loop\n",
        "def compute_cost_loop(X,y,w):\n",
        "  cost = 0\n",
        "  m, n = X.shape\n",
        "  for i in range(m):\n",
        "    fi = 0\n",
        "    for j in range(n):\n",
        "      fi += w[j] * X[i,j]\n",
        "    cost += (y[i] - fi)**2\n",
        "  #one half mse\n",
        "  cost = (1/(2*m))*cost\n",
        "  return cost\n",
        "\n",
        "#Vectorization\n",
        "def compute_cost_vector(X,y,w):\n",
        "  cost = (1/(2*m)) * (X@w - y).T @ (X@w - y)\n",
        "  return cost\n"
      ],
      "metadata": {
        "id": "WdisZdfCZF-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For loop\n",
        "def compute_gradient_one(X,y,w,k):\n",
        "  #calculate derivative of cost function with regard to specific coefficient k\n",
        "  m, n = X.shape\n",
        "  dev = 0\n",
        "  for i in range(m):\n",
        "    fi = 0\n",
        "    for j in range(n):\n",
        "      fi += w[j] * X[i,j]\n",
        "    dev += (fi - y[i])*X[i,k]\n",
        "  dev = dev/m\n",
        "  return dev\n",
        "\n",
        "def compute_gradient_loop(X,y,w):\n",
        "  m, n = X.shape\n",
        "  grads = []\n",
        "  for k in range(n):\n",
        "    dJdk = compute_gradient_one(X,y,w,k)\n",
        "    grads.append(dJdk)\n",
        "  return grads\n",
        "\n",
        "#Vectorization\n",
        "def compute_gradient_vector(X,y,w):\n",
        "  grads = (1/m) * X.T @ (X@w - y)\n",
        "  return grads"
      ],
      "metadata": {
        "id": "fgodcoDpdKHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For loop\n",
        "def gradient_descent_loop(X, y, initial_w, alpha, iters):\n",
        "  w = copy.deepcopy(initial_w)\n",
        "  cost_history = []\n",
        "  for iter in range(iters):\n",
        "    #Compute derivatives\n",
        "    grads = compute_gradient_loop(X, y, w)\n",
        "\n",
        "    #Gradient descent\n",
        "    for i in range(grads):\n",
        "      w[i] -= alpha * grads[i]\n",
        "\n",
        "    #Keep track of cost\n",
        "    cost = compute_cost_loop(X,y,w)\n",
        "    cost_history.append(cost)\n",
        "\n",
        "    #Print cost at intervals\n",
        "    if iter % math.ceil(iters/10) == 0:\n",
        "      print(f\"Iteration {iter:4}: Cost {float(cost):8.2f}\")\n",
        "\n",
        "  return w, cost_history\n",
        "\n",
        "\n",
        "#Vectorization\n",
        "def gradient_descent_vector(X, y, initial_w, alpha, iters):\n",
        "  w = copy.deepcopy(initial_w)\n",
        "  cost_history = []\n",
        "  for iter in range(iters):\n",
        "    #Gradient descent\n",
        "    w -= alpha * (1/m) * X.T @ (X@w - y)\n",
        "\n",
        "    #Keep track of cost\n",
        "    cost = compute_cost_loop(X,y,w)\n",
        "    cost_history.append(cost)\n",
        "\n",
        "    #Print cost at intervals\n",
        "    if iter % math.ceil(iters/10) == 0:\n",
        "      print(f\"Iteration {iter:4}: Cost {float(cost):8.2f}\")\n",
        "\n",
        "  return w, cost_history"
      ],
      "metadata": {
        "id": "wY0Oo_0Dh1IP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PYoEjojnmpFI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}