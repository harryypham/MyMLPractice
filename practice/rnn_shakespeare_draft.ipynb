{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "authorship_tag": "ABX9TyNPtsHbTLILXB5XuWe8Ugzq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harryypham/MyMLPractice/blob/main/practice/rnn_shakespeare_draft.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "!pip install tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BQpzE-8-NNp",
        "outputId": "ca3557dd-8bae-4c01-d051-f678ba693090"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-25 10:37:49--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.1’\n",
            "\n",
            "input.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.008s  \n",
            "\n",
            "2024-06-25 10:37:49 (127 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
            "\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vvMEolM6eUeg"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "sequence_length = 512\n",
        "num_layers = 3\n",
        "hidden_size = 1024\n",
        "emb_dim = 256\n",
        "batch_size = 64\n",
        "lr = 3e-4\n",
        "num_epochs = 5"
      ],
      "metadata": {
        "id": "E_-rNedT95AG"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt','r', encoding='utf-8') as f:\n",
        "  text = f.read()\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QCBV_v80Ikx",
        "outputId": "fd09348e-bdbc-4f53-8450-457855b8a77a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for i,ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])"
      ],
      "metadata": {
        "id": "tE1vzlhD0kh1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "batch_size = 64\n",
        "block_size = 512\n",
        "\n",
        "\n",
        "def get_batch():\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  return x, y\n",
        "\n",
        "xb, yb = get_batch()"
      ],
      "metadata": {
        "id": "dQ29hIMj0qID"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "  def __init__(self, sequence_length, hidden_size, emb_dim, num_layers, vocab_size):\n",
        "    super().__init__()\n",
        "    self.embedding_table = nn.Embedding(vocab_size, emb_dim)\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    self.lstm = nn.LSTM(emb_dim, hidden_size, num_layers, batch_first=True)\n",
        "    self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "  def forward(self, x, targets=None):\n",
        "    h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "    c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "\n",
        "    x = self.embedding_table(x)\n",
        "    logits, _ = self.lstm(x, (h0, c0))\n",
        "    if targets is not None:\n",
        "      B, T, C = logits.shape\n",
        "      logits = self.fc(logits.reshape(B*T, C))\n",
        "      targets = targets.view(-1)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "    else:\n",
        "      logits = self.fc(logits[:, -1, :])\n",
        "      loss = None\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self):\n",
        "    idx = torch.zeros((1,1), dtype=torch.long)\n",
        "    for _ in range(400):\n",
        "      idx = idx.to(device)\n",
        "      logits, loss = self(idx)\n",
        "\n",
        "      probs = F.softmax(logits, dim=1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "      idx = torch.cat([idx, idx_next], dim=1)\n",
        "    return idx"
      ],
      "metadata": {
        "id": "cSafRd0mBo_g"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, data_loader, num_epochs, device):\n",
        "  model.train()\n",
        "  losses = []\n",
        "  for epoch in range(1, num_epochs+1):\n",
        "    print(f\"Epoch {epoch}: \")\n",
        "    pbar = tqdm(data_loader, leave=True)\n",
        "    for batch_idx, (input, target) in enumerate(pbar):\n",
        "      input = input.to(device).squeeze(1)\n",
        "      target = target.to(device)\n",
        "\n",
        "      logits, loss = model(input)\n",
        "\n",
        "      losses.append(loss.item())\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "\n",
        "      pbar.set_postfix({\"Loss\": round(sum(losses)/len(losses), 4)})\n",
        "\n",
        "@torch.no_grad()\n",
        "def check_accuracy(model, data_loader, device):\n",
        "  correct = total = 0\n",
        "\n",
        "  model.eval()\n",
        "  for input, target in data_loader:\n",
        "    input = input.to(device).squeeze(1)\n",
        "    target = target.to(device)\n",
        "\n",
        "    output = model(input)\n",
        "\n",
        "    _, preds = output.max(1)\n",
        "    correct += (preds == target).sum()\n",
        "    total += target.size(0)\n",
        "\n",
        "  print(f\"Accuracy: {correct/total*100:.2f}\")\n",
        "  model.train()\n"
      ],
      "metadata": {
        "id": "user1Tfv-tbf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LSTM(sequence_length, hidden_size, emb_dim, num_layers, vocab_size).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "losses = []\n",
        "for iter in range(1500):\n",
        "  inputs, targets = get_batch()\n",
        "  inputs = inputs.to(device)\n",
        "  targets = targets.to(device)\n",
        "\n",
        "  logits, loss = model(inputs, targets)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  losses.append(loss.item())\n",
        "  if iter % 50 == 0:\n",
        "    print(\"iter\", iter, sum(losses)/len(losses))\n",
        "\n",
        "  if iter != 0 and iter % 1000 == 0:\n",
        "    for g in optimizer.param_groups:\n",
        "      g['lr'] = 3e-5\n",
        "\n",
        "  optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjnnnV9mDEOU",
        "outputId": "e46a9018-3585-4821-b758-e4e351d63f79"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter 0 4.177000999450684\n",
            "iter 50 3.320663475522808\n",
            "iter 100 2.978285305570848\n",
            "iter 150 2.718244893661398\n",
            "iter 200 2.5318225100265805\n",
            "iter 250 2.3888451421403314\n",
            "iter 300 2.2735725494714276\n",
            "iter 350 2.1782456979452713\n",
            "iter 400 2.097607871243484\n",
            "iter 450 2.0282790906677755\n",
            "iter 500 1.9677050760882104\n",
            "iter 550 1.9146102560842968\n",
            "iter 600 1.867792470879642\n",
            "iter 650 1.8251316135380125\n",
            "iter 700 1.786496987186383\n",
            "iter 750 1.7513975071684815\n",
            "iter 800 1.719051100043917\n",
            "iter 850 1.6888270036314126\n",
            "iter 900 1.6607740118553846\n",
            "iter 950 1.6340923724239682\n",
            "iter 1000 1.609220661721625\n",
            "iter 1050 1.5850331475913921\n",
            "iter 1100 1.562580522356198\n",
            "iter 1150 1.5417856807609314\n",
            "iter 1200 1.522591334397747\n",
            "iter 1250 1.50456757720807\n",
            "iter 1300 1.487849196134211\n",
            "iter 1350 1.472332255916186\n",
            "iter 1400 1.4576667875668392\n",
            "iter 1450 1.4438576870832174\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(model.generate()[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0Jsb6sbIP-y",
        "outputId": "d4d83698-82ad-4108-80b5-e5e5d5b13582"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Be sits fair to touch the Clarence of his bent?\n",
            "\n",
            "SICINIUS:\n",
            "Dar'sness grows,\n",
            "A beg of vilaging! I am teldow, my father.\n",
            "\n",
            "VELERIA:\n",
            "Prey, into thy clouds! what is it not?\n",
            "\n",
            "First Gentleman:\n",
            "Aming but it ears you ever since the word\n",
            "Upon that broads 'long by the instrument:\n",
            "They that lay hath something it as time\n",
            "Swell commending pretition,\n",
            "And ask along to me.\n",
            "\n",
            "Second Murderer:\n",
            "Speak no me, did I resi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GlIuLyvJKAQl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}