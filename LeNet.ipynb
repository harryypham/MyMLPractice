{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fcEwIiuZEgvv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mean_and_std(dataset, num_channels):\n",
        "    \"\"\"\n",
        "    Calculate the mean and std of a dataset\n",
        "    \"\"\"\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True, num_workers=2)\n",
        "    mean = torch.zeros(3)\n",
        "    std = torch.zeros(3)\n",
        "    for inputs, labels in dataloader:\n",
        "      for channel in range(num_channels):\n",
        "        mean[channel] += inputs[:,channel,:,:].mean()\n",
        "        std[channel] += inputs[:,channel,:,:].std()\n",
        "    mean.div_(len(dataset))\n",
        "    std.div_(len(dataset))\n",
        "    return mean, std\n",
        "\n",
        "#Dummy dataset to get mean and std\n",
        "temp_dataset = torchvision.datasets.CIFAR10(root=\"/content/data\", train=True, download=True, transform=transforms.ToTensor())\n",
        "mean, std = get_mean_and_std(temp_dataset, 3)\n",
        "mean = tuple(mean.numpy())\n",
        "std = tuple(std.numpy())\n",
        "\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "train_dataset = torchvision.datasets.CIFAR10(root=\"/content/data/train\", train=True, download=True, transform=transform_train)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root=\"/content/data/test\", train=False, download=True, transform=transform_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dREJiGtwFSxa",
        "outputId": "c1b68eed-ed2b-46ca-e294-b297e3ecb990"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /content/data/train/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:03<00:00, 43157810.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/data/train/cifar-10-python.tar.gz to /content/data/train\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /content/data/test/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:04<00:00, 42090831.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/data/test/cifar-10-python.tar.gz to /content/data/test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "batch_size = 64\n",
        "learning_rates = [0.05, 0.01, 0.005, 0.001]\n",
        "activations = [\"relu\", \"tanh\"]\n",
        "pools = [\"max\", \"average\"]\n",
        "optimizers = [\"Adam\", \"SGD\"]\n",
        "epochs = 10\n",
        "\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer','dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "metadata": {
        "id": "2p9Eyu__E2k1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LeNet(nn.Module):\n",
        "  def __init__(self, act_func=\"relu\", pool_type=\"max\"):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 6*3, 5)\n",
        "    self.conv2 = nn.Conv2d(6*3, 16*3, 5)\n",
        "    self.pool1 = self.pool(pool_type)\n",
        "    self.pool2 = self.pool(pool_type)\n",
        "    self.fc1   = nn.Linear(1200, 120)\n",
        "    self.fc2   = nn.Linear(120, 84)\n",
        "    self.fc3   = nn.Linear(84, 10)\n",
        "    self.act_func = act_func\n",
        "    self.pool_type = pool_type\n",
        "\n",
        "  def pool(self, pool_type=\"max\"):\n",
        "    if pool_type == \"average\":\n",
        "      return nn.AvgPool2d(2)\n",
        "    else:\n",
        "      return nn.MaxPool2d(2)\n",
        "\n",
        "  def act(self, x, act_func=\"relu\"):\n",
        "    if act_func == \"tanh\":\n",
        "      return F.tanh(x)\n",
        "    else:\n",
        "      return F.relu(x)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.act(self.conv1(x), self.act_func)\n",
        "    x = self.pool1(x)\n",
        "    x = self.act(self.conv2(x), self.act_func)\n",
        "    x = self.pool2(x)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = self.act(self.fc1(x), self.act_func)\n",
        "    x = self.act(self.fc2(x), self.act_func)\n",
        "    x = self.fc3(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "JfLbtX-LLpFc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_accuracy_test(model, dataloader, device):\n",
        "    \"\"\"\n",
        "    Compute accuracy on test set\n",
        "    \"\"\"\n",
        "    correct, total = 0, 0\n",
        "    for inputs, targets in dataloader:\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predict = outputs.max(1)\n",
        "\n",
        "        total += targets.size(0)\n",
        "        correct += predict.eq(targets).sum().item()\n",
        "\n",
        "    return correct/total * 100"
      ],
      "metadata": {
        "id": "enG2_TeFR-0i"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, trainloader, testloader, criterion, optimizer, epochs, device, compute_accuracy_test):\n",
        "  loss_history, acc_history = [], []\n",
        "  for epoch in range(epochs):\n",
        "    print(f'\\nEpoch {epoch+1}:')\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predict = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predict.eq(targets).sum().item()\n",
        "        if (not batch_idx % 150) and batch_idx != 0:\n",
        "              print ('Batch %03d | Cost: %.6f | Accuracy: %.4f'\n",
        "                    %(batch_idx, train_loss/(batch_idx+1), 100*correct/total))\n",
        "\n",
        "    loss_history.append(train_loss/(batch_idx+1))\n",
        "    acc_history.append(100*correct/total)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.set_grad_enabled(False):\n",
        "        test_acc = compute_accuracy_test(net, testloader, device)\n",
        "\n",
        "  return loss_history, acc_history, test_acc"
      ],
      "metadata": {
        "id": "z404HZsqQ7lb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def subplot_act_pool(axs, idx):\n",
        "  for key, value in history.items():\n",
        "    xs = [x+1 for x in range(len(value[idx]))]\n",
        "    axs.plot(xs, value[idx], label=key+\" pool\")\n",
        "  axs.legend()\n",
        "  axs.set_xlabel(\"Epoch\")\n",
        "  if idx == 0:\n",
        "    axs.set_ylabel(\"Training loss\")\n",
        "  if idx == 1:\n",
        "    axs.set_ylabel(\"Accuracy\")\n",
        "\n",
        "def plot_act_pool(history):\n",
        "  \"\"\"\n",
        "  Plot combinations of activation functions and pooling methods\n",
        "  \"\"\"\n",
        "  fig, axs = plt.subplots(1,2, figsize=(12, 5))\n",
        "  for i in range(2):\n",
        "    subplot_act_pool(axs[i], i)\n",
        "  plt.annotate('Train with Adam optimizer using constant learning rate=0.001 and weight decay=5e-4',\n",
        "              xy = (1.0, -0.2),\n",
        "              xycoords='axes fraction',\n",
        "              ha='right',\n",
        "              va=\"center\",\n",
        "              fontsize=10)\n",
        "  fig.show()\n",
        "\n",
        "def subplot_opt_lr(axs, key, value):\n",
        "  xs = [x+1 for x in range(10)]\n",
        "  axs.plot(xs, value[\"Adam\"][0], label=\"Adam\")\n",
        "  axs.plot(xs, value[\"SGD\"][0], label=\"SGD with momentum\")\n",
        "  axs.text(xs[-1], value[\"Adam\"][0][-1], '{:.3f}'.format(value[\"Adam\"][0][-1]))\n",
        "  axs.text(xs[-1], value[\"SGD\"][0][-1], '{:.3f}'.format(value[\"SGD\"][0][-1]))\n",
        "  axs.legend()\n",
        "  axs.set_xlabel(\"Epoch\")\n",
        "  axs.set_ylabel(\"Training loss\")\n",
        "  axs.set_title(f\"Learning rate {key}\")\n",
        "\n",
        "def plot_opt_lr(history):\n",
        "  \"\"\"\n",
        "  Plot combinations of different optimizers and learning rates\n",
        "  \"\"\"\n",
        "  fig, axs = plt.subplots(2,2, figsize=(12, 10))\n",
        "  for i, (key, value) in enumerate(history.items()):\n",
        "    subplot_opt_lr(axs[i//2, i%2], key, value)\n",
        "  fig.show()\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "kcdgjlG6YToH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Compare between different learning rates and optimizers\n",
        "history = {}\n",
        "for lr in learning_rates:\n",
        "  opt_dict = {}\n",
        "  for opt in optimizers:\n",
        "\n",
        "    net = LeNet()\n",
        "    net.to(device)\n",
        "    if device == \"cuda:0\":\n",
        "      net = nn.DataParallel(net)\n",
        "\n",
        "    if opt == \"Adam\":\n",
        "      optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=5e-4)\n",
        "    if opt == \"SGD\":\n",
        "      optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    loss_history, acc_history, test_acc = train(net, trainloader, testloader, criterion, optimizer, epochs, device, compute_accuracy_test)\n",
        "\n",
        "    opt_dict[opt] = (loss_history, acc_history, test_acc)\n",
        "  history[str(lr)] = opt_dict\n",
        "\n",
        "plot_opt_lr(history)"
      ],
      "metadata": {
        "id": "DRLp8RbXdD8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Compare between different pooling methods and activation functions\n",
        "history = {}\n",
        "for activation in activations:\n",
        "  for pool in pools:\n",
        "    net = LeNet(act_func=activation, pool_type=pool)\n",
        "    net.to(device)\n",
        "    if device == \"cuda:0\":\n",
        "      net = nn.DataParallel(net)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(net.parameters(), lr=learning_rates[2], weight_decay=5e-4)\n",
        "\n",
        "    loss_history, acc_history, test_acc = train(net, trainloader, testloader, criterion, optimizer, epochs, device, compute_accuracy_test)\n",
        "\n",
        "    history[f\"{activation} + {pool}\"] = (loss_history, acc_history, test_acc)\n",
        "\n",
        "plot_act_pool(history)"
      ],
      "metadata": {
        "id": "1MnBGMvUP0Bh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7iSgDARhaceX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}